{"title":"Yikun","description":null,"language":null,"link":"http://yikun.github.io","pubDate":"Wed, 06 Dec 2017 01:41:11 GMT","lastBuildDate":"Fri, 08 Dec 2017 12:01:03 GMT","generator":"hexo-generator-json-feed","webMaster":"Yikun","items":[{"title":"Nova调度相关特性理解与梳理","link":"http://yikun.github.io/2017/12/06/Nova调度相关特性理解与梳理/","description":"准备拿这篇文章梳理下OpenStack Nova调度相关的特性，由于目前Placement的引入，说起调度，和这个组件是分不开的，所以本文也可以看做是Placement的一个历史特性的梳理。第一阶段会按照版本，先把调度相关的BP过一遍，然后再通过理解和使用加强理解。好吧，我承认又开了一个系列的坑，话不多说，开始！","pubDate":"Wed, 06 Dec 2017 01:41:11 GMT","guid":"http://yikun.github.io/2017/12/06/Nova调度相关特性理解与梳理/","category":"Nova,OpenStack"},{"title":"跨Cell场景下查询的那些事儿","link":"http://yikun.github.io/2017/11/16/跨Cell场景下查询的那些事儿/","description":"1. 背景我们知道Nova目前正在慢慢地演进到Cell V2架构，Cell V2架构中，很重要的一个变化就是数据库的拆分，清晰的划分了数据库的职能，从而有具备横向扩展的能力。顶层数据库(nova_api)用来存储全局数据，而Cell中的数据库(nova_cellX)仅存储计算节点相关的数据。比如，创建虚拟机的全局数据，比如Flavor、Keypair之类的数据，放在上层的nova_api数据库中，而虚拟机本身的信息，比如某个虚拟机的信息，放在了子Cell中。 这样的架构另一个好处是Cell很轻松的可以实现扩展，从而提升虚拟机数量的规模。然而，这引入了一个问题，就是没有一个地方存储着全量虚拟机的数据了。当我们需要一些全局的虚拟机数据查询时（比如查询全量虚拟机列表）就比较棘手了。 2. 数据库的拆分其实这样的架构在目前互联网业务中十分常见，随着业务量和历史数据的增长，很多业务都需要进行分表分库，切分的目的主要有2个，一是单个数据库的存储空间已经不足以支撑庞大的数据量，另外一个是单个数据库所能承载的连接数或者并发数不足以满足逐渐飙升的请求。一般来说，数据库的分库为垂直分库和水平分库。 垂直分库。一般按照功能划分，每个分库的功能不同。把不同功能查询或写入的负载均分到独立功能库中。例如，我们将一些基础信息独立成一个库，详细信息独立成一个库，这种按照功能的划分，将负载均衡，只需要基础信息的去访问基础库，需要详细信息的时候，再去查详细信息的库。在Cell V2的架构中，我们可以将nova_api和nova_cellX的数据库划分看成是垂直划分，对于nova_api库来说，只用关心上层全局数据的存储处理，而对于nova_cell库来说需要关心的是每个子cell里面数据的存储处理。 水平分库。一般通过某种方法把数据打散到不同的库中，每个库的表结构是相同的。例如，我们根据用户ID进行分库，可以通过映射表、取余、Hash的方式来确定某个用户的请求到底落到哪个数据库去查。在Cell V2的架构中，每个Cell数据的划分，就可以看做是水平分库了，虚拟机按照一定的“规则”，落到了不同的Cell中。 一般的业务演进，一般是先进行垂直分表分库，然后当用户或者数据规模达到一定程度后，再通过水平分库提升规模。就像OpenStack Nova一样，最开始所有的数据都聚集在一个叫Nova的库里，然后拆分出nova_api和nova，最后再将nova_cell拆出来，并根据虚拟机ID和Cell进行mapping，从而完成水平分库。 2. 引入的问题 在Cell V2的场景下，对于单个虚拟机来说基本没什么变化，无非就是多了一个映射查询的步骤：先查找虚拟机所对应的Cell数据库，然后对这个数据库进行操作即可。 然而，由于水平切分导致每个Cell都丢失了“全局视角”，例如之前我们进行虚拟机列表查询时，在原来只需要在一个数据库查询，现在需要在多个数据库查询，尤其是需要指定一些全局参数进行查询时，比如limit、marker、sort等参数一加上，就更恶心了。 3. 解决方案这个问题，在多Cell场景一直没有得到很好的解决，只是在各个Cell里面搜集各个cell排序好的数据，然后append到结果里面，也并没有进行最终排序。呈现给用户来看，就是数据是乱序的。 直到Q版本，Dan Smith大神提了一系列的instance-list Patch)，初步解决了这个问题。 我们先看看他的实现，这一系列的Patch最关键的实现在instance_list.py中，大致的撇一眼整体的实现，全局/局部marker、并行搜集跨Cell数据、heap归并排序，相信从这几个关键词中你已经猜到了实现的大概逻辑。下面，我们先整体介绍一下整体的实现逻辑，然后再对实现细节做详细的解释。 我们举个例子，来看看在跨cell场景下，如果获取一个虚拟机的列表 假设我们有2个cell，里面有编号为i0~i5的6个虚拟机，从i0到i5，按顺序依次创建。当我们查询全量虚拟机时，按照创建时间逆序，我们期待得到的结果是i5 i4 i3 i2 i1 i0。再传递limit=2/sort=orderby(created_at,desc)/marker(i5)之后，处理的过程如下： Step 1 查找全局marker。我们首先需要在各个Cell中查找，i5是否存在。对于虚拟机来说，比较简单，在nova_api数据库中存在instance_mappings表，这个表里面记录了虚拟机和cell的映射关系，如果marker在cell中，我们直接可以从mapping表中找到它及其对应的cell。我们的例子中，我们找到了位于cell2中的i5。 参考代码：nova/compute/instance_list.py#L134,L142具体获取marker时，是先在instance_mapping中查找了marker所在的cell，然后，在target_cell拿到了marker的信息，参考代码nova/compute/instance_list.py#L66,L88。 Step 2 并行查询子Cell中的local marker，并获取满足条件的虚拟机。 在第一步中，我们拿到了i5的信息，由于排序是按照created_at，逆序，那我们查询local marker的时候，只需要一条SQL就可以拿到Cell中满足条件的local marker：1SELECT * FROM instances ORDER BY created_at DESC limit 1 然后，我们再根据这个local marker就可以拿到满足条件的虚拟机列表了。当然，我们需要各取limit个，因为也许最终满足条件的都在一个cell中，所以，我们取得的虚拟机列表中虚拟机的个数应该按照全局limit来获取。值得注意的是，local marker如果不是global marker的话，我们需要把local marker也算在满足条件的列表中，因为全局来看，这个marker也是满足用户条件的。如果local marker也刚好就是global marker，那这个marker就不用管了。参考代码nova/compute/instance_list.py#L166,L212，先找到满足条件的第一个local marker，然后就按照之前的流程，获取marker之后的记录就可以了。 Step 3 合并排序。在步骤2中，我们会得到2个列表，分别是从cell1和cell2中拿到的数据。这两个列表是有序的，但是当我们合并后，需要进行全局重排序。这样，我们就拿到了有序的列表。 参考代码nova/compute/instance_list.py#L228,L234，可以看到，由于每个cell拿到的数据都是有序的，因此，最终排序的时候，也是用了heapq这个数据结构，来高效的完成有序列表的合并排序。 Step 4 进行limit操作。最后，我们在第三步的结果上，进行limit操作就好了。在合并的过程中，对limit进行检查，最终，完成limit且排好序的结果。 4. The end, the begin.Instance list给我们示范了一个跨Cell场景下列表的实现，主要有2个地方很有亮点，一个是并行的查询各个Cell的数据，一个是最终排序选择了heapq作为排序的结构，在保证分页正常的情况下，也兼顾了性能。比较残忍的是，随着跨Cell迁移的支持，几乎后续所有和子Cell数据相关的列表查询，都需要进行跨Cell的支持，比如migration、instance action。 架构很丰满，实现很骨感。不多说了，滚去写cross cell support的patch了。：）","pubDate":"Thu, 16 Nov 2017 13:19:14 GMT","guid":"http://yikun.github.io/2017/11/16/跨Cell场景下查询的那些事儿/","category":"Nova,OpenStack"},{"title":"[译] An Update on the Placement API and Scheduler plans for Queens","link":"http://yikun.github.io/2017/10/25/译-An-Update-on-the-Placement-API-and-Scheduler-plans-for-Queens/","description":"原文链接：https://github.com/jaypipes/articles/blob/master/openstack/placement-queens-update.md 这篇文章主要讲了在过去几个版本中，OpenStack社区对于Nova调度及Placement服务相关工作的更新进展。我也会着重介绍一些我们在Q版本中主要处理的几个BP，同时也介绍了未来重点工作的路标，我们会在未来的几个release中完成它们。 1. 过去版本完成情况的总结Placement API已经在OpenStack的N版本成为一个独立的API endpoint。 Placement API将存量记录(tracking inventory)、资源消耗(resource consumption)、资源的分组和共享(grouping and sharing of resources)以及表示资源所具能力字符串标签（string capability tags, traits）这些数据暴露出来。 从那之后，社区持续地改进API，并且在Nova中更进一步地进行集成。 在Newton版本，主要是让nova-compute正确地对本地的资源进行盘点，然后把这些inventory记录送到Placement API中。 在Ocata版本，我们开始将nova-scheduler服务与Placement API进行集成。我们在scheduler进行了一些修改，使用Placement API进行满足一些基本资源请求条件的计算节点过滤。我们也添加了aggregates，来提供resource provider的分组机制。 在Pike版本，我们主要将资源claim这个步骤从nova-compute移动到nova-scheduler中。做这个事主要有2个原因：一个是性能/扩展以及和Cells V2架构适配。在”候选主机列表及cell中重试”一节中，进行了详细介绍。 2. Queens版本的优先级在丹佛的PTG中，Nova社区团队决定了在调度和资源placment功能中3个主要的工作： 妥当地处理迁移操作 在多cell中的备选主机列表及重试操作 嵌套的resource provider 应该指出的是，我们理解在这个领域仍然有许多许多的新功能需求，一些已经在我们的“雷达”中很多年了。我们承认当一些管理员或者一些潜在的用户看到一些长时间存在的问题或者工作并没有出现在Queens版本的高优先级列表中，会有点沮丧。然而，core团队实际能够review的东西是有限的，所以我们必须做这些决策。当然，我们也非常欢迎大家在PTG和邮件列表里来讨论这些决策。 另外一个应该指出的是，尽管在Q版本的scheduler和resource placement领域中，仅有这3个高优先级工作，但这并不意味着其他的工作不能被review或者推进。这个仅代表core团队将重点review这些领域的patch。 3. 正确的处理移动操作我们在Q版本第一优先级需要处理的是做与placement API相关的move操作（比如migrate、resize、evacuate、unshelve等）的收尾及测试用例的全覆盖。 在Pike版本所剩时间不多时，Balasz Gibizer、Dan Smith和Matt Riedemann发现了一系列问题：在做Nova支持的各种各样的move操作时，资源应该如何在Placement API中记录。在Pike版本的时候，我们开始在nova-scheduler服务里进行资源的claim。很显然我们也需要在move操作的时候进行claim处理。在最初版本的实现，在虚拟机迁移的过程中，我们为虚拟机创建了一个成对的allocation，源节点和目的节点的资源都会占用一条allocation记录。这样做，可以工作，但是这个方案明显有弊端，尤其是在我们在同节点进行resize的操作。 这产生了一系列关于在迁移时产生错误的或者丢了一些allocation记录的问题，我们需要在resource tracker和compute manger中，塞了很多不优雅的代码，来处理在滚动升级中新的conductor和scheduler服务的兼容，并且旧的计算节点也写入了不正确的记录。 Dan Smith也提出了一个解决在迁移操作中记录allocation问题的方法migration-allocations，但是因为时间问题，我们没办法在Pike版本实现。 在Queens版本，我们需要优选Dan的解决方案，即在迁移之前，在allocation记录中，将源节点的UUID替换为迁移object自己的UUID。这样允许目的节点使用虚拟机的UUID来占用allocation，一旦迁移成功，我们便可仅仅删除被migration UUID消费的allocation就行了。再也不用受双倍的allcations困扰了。 4. 候选主机列表及cell中重试第二优先级的事情是，我们应该在Cells V2部署中支持创建虚拟机请求的重试能力。 上面，我已经说过了，将资源claim的过程从compute移动到scheduler中，有两个原因，让我们来仔细讨论一下。 首先，现在的版本存在一个问题：2个scheduler进程为2个虚拟机选择同一个host时，当启动虚拟机先第一个host启动完成时，消耗了host的最后一点资源。然后，不幸的第2个启动进程，必须进行重新调度流程。这个流程有点太重了，sheduler必须通过通过RPC来为虚拟机获取一个新的目的节点，然后许多状态都需要通过request spec来传递。补充一下，并没有什么能够确保新的节点就一定可用，可能同样的命运将降临，再一次触发了重试。 在nova-scheduler代替目的计算节点来进行资源claim操作，意味着我们可以显著的减少在compute节点进行claim的时间和复杂性（造成重试操作的主要原因是在计算节点claim资源的竞争和竞态条件） 现在我们尝试在scheduler服务选择目标主机的时候，进行资源的claim操作。如果Placment API返回200 OK，我们就知道虚拟机已经在目的节点已经占用了这个资源，唯一可能造成重试的操作就是某些不正常的主机失败，即不是常见的失败原因。如果Placement API返回409冲突，我们可以从返回的error中看到失败原因，到底是因为并发刷新失败了，还是说目的节点确实没有足够的空间来容纳虚拟机。 如果另外的进程在目标主机完成资源claim的时间介于虚拟机调度选择和尝试claim资源之间，我们会简单的重试（在scheduler代码的小循环）尝试在目的主机claim资源。如果目的主机资源耗尽了，scheduler会选择另外的目的主机。我们完成这些时，不会启动的请求发送到目标的compute主机。 我们将资源的claim移动到scheduler的第二个原因，是因为Cells V2架构。再次说明，Cells V2架构移除了独立分离又略显奇葩的Cells V1旧代码。单单使用的Cells V2 API控制面，意味着能够更简单也更容易地进行代码维护。 然而，Cells V2设计架构的一个原则是一个启动（或者移动）虚拟机请求会获取到目标的cell，他没有向上调用的能力与API层进行通信。这对于我们目前的重试机制来说是一个问题。当前的重试机制依靠失败的计算节点来初始化资源的claim，并且能够反向调用scheduler，来找到另外的host进行虚拟机的启动。 Ed Leafe正在Quees版本努力，让scheduler从API/Scheduler传递一系列备选主机和allocation到目标的cell。这个备选主机和allocation信息，将会被cell的conducotr用来去那些备选的目的节点去重试，虚拟机资源的claim依靠备选host，无需访问上层的API/Scheduler。 5. 嵌套的resource providers第三优先级的事情是我们称之为”嵌套的resource providers”的东西。 例如，NUMA Cells和包含它的主机，SR-IOV网卡功能和包含它的主机，物理GPU组和包含它的主机。 让我们举个2个计算节点的例子，2个节点每个都含有2个SR-IOV网卡。第一个计算节点每个网卡都有8个虚拟机网卡，第二个计算节点其中的一个物理网卡被标记为直通的（意味着用户能够全面掌控）另外一个网卡则被指为8个虚拟机网卡，与第一个计算节点类似。 目前，Placement API无法理解父子provider这层关系。嵌套的resource provider这个spec和patch让Placement服务能够感知到这些，也允许用户区分子资源provider的父资源provider的UUID。 嵌套的resource provider开启了一系列的功能，包括PCI设备、高级网络、NUMA等的支持。正因如此，我们将共享的resource provider在Q版本放在了相对来说不重要的位置，然后更专注实现基本的嵌套resource provider，至少支持SR-IOV物理功能和虚拟功能的关系。 6. Queens其他的工作事项除了上面的优先级事项，我们也将投入精力去做其他的一些事物。尽管review将专注在上述的优先级高的事物中，我们也将在下述的几个方面尽可能地进行review。 (1) 完成trait-flavor这个是从Pike版本开始出现需要完成的工作。Placement API现在支持traits列表，简单的tags字符串来描述一个resource provider的能力。 然而，一些地方仍然需要编码完成。 Flavor中需要包含需要的traits列表 Scheduler需要向Placement API过滤那些某个flavor要求的所有traits的providers Virt drivers需要开始上报traits给计算节点的resource provider，来取代之前上报一个无结构的virt-driver-specific在virt driver的get_available_resource。 (2) Ironic virt driver的traits处理如上面提到的，virt driver需要开始上报traits信息给计算节点的resource provider。然而，ironic driver有点不同，因为他处理了多个计算节点的resource provider记录（有一个resource provider记录着部署的每个Ironic裸机节点） Jhon Garbutt在主要负责Ironic API支持在virt driver中上报traits。 (3) 在Placement API中缓存header处理Chris Dent提出了在Placement API中的一些资源endpoints增加”Last-Modified”和其他的HTTP头部。确保在cahcing代理下的正确行为非常重要，并且完成这项工作的工作量似乎是可以控制的。 (4) 在Placement API中支持POST多个allocations这个spec实际上是为了move操作的清理而开启的。Chris提出允许PORST /allocations调用（我们目前支持PUT /allocations/{consumer_uuid}调用）来支持在一个请求中写入多个consumers的多条allocation记录。这个将允许我们完成allocation从instance到migrations UUID的转换，这个是Dan Smith做move操作资源跟踪方案的一部分。 (5) 初步支持vGPU尽管这个不像从Citrix的Jianghua Wang提出的spec来完全实现vGPU，在Queens版本，我们仍将尝试至少完成vGPU资源的基本支持。 基础的支持意味着可能不会支持多GPU类型或者pGPU池（换句话说，仅支持每个计算节点的VGPU有一个单一的inventory记录）。 7. Beyond Queens(1) 一个通用的设备管理Eric Fried和我正在讨论一个generic device manager，将会在一寸的nova/pci模块中替换很多代码。我们可能最早在Rocky版本完成。 (2) 支持NUMA尽管嵌套的resource provider是为NUMA拓扑设计的，实际上，想要能够通过Placment API同等功能的把NUMATopologyFilter在Nova scheduler取代，仍然还有很长时间，可能在Rokcy版本吧。 在Nova支持NUMA的实现与支持大页内存、CPU pining、模拟IO线程pinning甚至是PCI设备管理（比如PCI设备的NUMA亲和性）是强耦合的。 似乎在可以看到的将来，NUMATopologyFilter仍然在Nova Scheduler保留，作为一个复杂自定义的调度failer/weigher，我们会慢慢的修改virt driver接口和resource tracker在nova-compute节点向Placement API上报NUMA cells信息作为resource provider。渐渐地通过查询Plament database来替换一些NUMATopologyFilter的功能。 (3) 共享的resource providerPlacement API允许resource provider通过aggregate关联来和其他provider来共享资源。这些reousrce provider我们称之为”shared resource providers”，尽管使用”sharing resource providers”更能合适地表达其目标。 我们需要为共享存储和路由网络IP池的用力完成和增加一些功能测试，确保资源上报和跟踪正确的完成。","pubDate":"Wed, 25 Oct 2017 06:10:07 GMT","guid":"http://yikun.github.io/2017/10/25/译-An-Update-on-the-Placement-API-and-Scheduler-plans-for-Queens/","category":"Nova,OpenStack"},{"title":"OpenStack Nova虚拟机冷迁移流程解析","link":"http://yikun.github.io/2017/10/11/OpenStack-Nova虚拟机冷迁移流程解析/","description":"1. 概述虚拟机冷迁移由于当用户想把虚拟机从一个计算节点移动到其他节点。主要涉及的命令如下：12$ nova migrate server_id$ nova resize-confirm server_id 看到后是不是觉得有点奇怪为啥migrate之后，还要resize-confirm？resize操作其实和migrate操作比较类似，不同的是迁移前后的flavor不一样。一般情况下resize的场景是，对虚拟机进行扩容，把flavor调大之类的。所以，在代码级别，nova也将两个流程合一了。migrate就是一个没有flavor变化的resize。 2. 核心流程下图是虚拟机冷迁移时，涉及的组件交互：我们可以看到，在迁移时，主要流程包括调度、迁移准备、迁移、完成迁移。 调度。conducotr通过select_destination访问Scheduler进行调度，最终选择一个可用的目的节点； prep_resize阶段，迁移准备。在目的主机上进行一些检查，比如是否支持相同节点迁移、虚拟机的主机等检查。然后，生成了一个resize claim（不是真正的claim，目前看这个只是刷新的resource tracker的一些数据），并在Placment刷新了inventory信息。 resize_instance阶段，在源节点把网络、磁盘之类的断掉，并且将数据复制到目的节点； 迁移结束。在目的节点配置网络、挂卷，启动虚拟机； (resize-confirm)确认迁移。使用nova resize-confirm确认删除，把原来网络断掉，并完成源虚拟机数据及中间虚拟机数据的清空。 具体细节，包括迁移的状态变化，如下图所示：","pubDate":"Wed, 11 Oct 2017 08:36:09 GMT","guid":"http://yikun.github.io/2017/10/11/OpenStack-Nova虚拟机冷迁移流程解析/","category":"Nova,OpenStack"},{"title":"OpenStack Nova虚拟机创建流程解析","link":"http://yikun.github.io/2017/09/27/OpenStack-Nova虚拟机创建流程解析/","description":"1. 概述Nova是OpenStack中处理计算业务（虚拟机、裸机、容器）的组件，整体的虚拟机创建流程自然是学习和熟悉Nova组件的第一步。本篇文章主要基于OpenStack Pike版本，基于最新的Cell v2架构部署为例，来介绍虚拟机的创建流程，并分析了Pike等最近几个版本中，虚拟机创建流程的关键变化。 2. 虚拟机创建流程 上图是虚拟机创建流程的整体流程，可以看到整体虚拟机创建流程一次经过了API、Conductor、Scheduler、Placement、Compute等主要服务，下面我们逐步介绍下虚拟机创建时，这些服务做的一些事情以及在Pike版本新引入的部分： 2.1 Nova-API在OpenStack的组件中，基本每个组件都会有一个API服务，对于Nova来说API服务主要的作用就是接收由用户通过Client或者一些其他REST请求工具（比如 curl、postman）发送的请求。一般来说会包含一些虚拟机创建的参数，比如虚拟机的规格、可用域之类的信息。 在虚拟机创建的流程中，API就是Nova的入口，当API接收到请求后，主要会处理一些关于参数校验、配额检测等事务。 1. 参数校验例如，我们指定镜像和规格来创建一个虚拟机时，通常会使用：1nova --debug boot --image 81e58b1a-4732-4255-b4f8-c844430485d2 --flavor 1 yikun 我们通过--debug来开启debug模式，来看看命令行究竟做了什么事，可以从回显中，看到一个关键的信息： curl -g -i -X POST http://xxx.xxx.xxx.xxx/compute/v2.1/servers -H “Accept: application/json” -H “User-Agent: python-novaclient” -H “OpenStack-API-Version: compute 2.53” -H “X-OpenStack-Nova-API-Version: 2.53” -H “X-Auth-Token: $token” -H “Content-Type: application/json” -d ‘{“server”: {“name”: “yikun”, “imageRef”: “81e58b1a-4732-4255-b4f8-c844430485d2”, “flavorRef”: “1”, “max_count”: 1, “min_count”: 1, “networks”: “auto”}}’ 我们可以看到虚拟机创建时，传入了一些诸如虚拟机名称、镜像、规格、个数、网络等基本信息。在API中，首先就会对这些参数进行校验，比如镜像ID是否合法、网络是否正确等。 2. 配额检测值得一提的是，在Pike版本的虚拟机创建开始时，对配额检测进行了优化。 我先看看之前的实现，在之前版本的Nova中，Quota检测过程相对来说比较复杂，首先会进行reserve操作，对资源进行预占，然后预占成功，并且创建成功后，最终会进行commit操作。然而，为了保证在并发的场景下，不会对超过用户配额（这都是钱啊！），因此在reserve和commit进行资源更新的时候都会quota相关的数据表的用户相关行加把锁，也就是说更新quota记录的时候，一个用户去更新时，其他用户再想刷新只能等着，直到前一个用户完成数据库记录刷新为止，这样就大大降低的效率，并发的性能也就不是很客观了。 另外，由于需要对cell v2进行支持，目前所有的quota表均已移动到API的数据库了可以参考BPCellsV2 - Move quota tables to API database。Cell V2的设计思想是，由API、Super Conductor去访问上层的全局数据库（nova_api数据库），而底下的cell中的组件，只需要关心cell中的逻辑即可。因此，为了彻底的解耦，让cell中的compute无需再访问api数据库进行诸如commit操作，在Pike版本，社区对quota机制进行了优化，详情可以参考Count resources to check quota in API for cells这个BP。 因此Pike版本之后，配额检测变成了这样： 首先，api中进行第一次Quota检测，主要方法就是收集地下各个cell数据库中的资源信息，然后和api数据库中的quota上限进行对比。例如，一个用户可以创建10个虚拟机，在cell1中有2个，cell2中有7个，再创建一个虚拟机时，会搜集cell1和cell2中的虚拟机个数之和（9个），然后加上变化（新增一个），与总配额进行比较。 二次检测（cell v2在super conductor里做）。由于在并发场景下，可能出现同时检测发现满足，之后进行创建，就会造成配额的超分，针对这个问题，社区目前给出的方案是，在创建虚拟机记录之后，再进行recheck，如果发现超额了，会将超额分配的虚拟机标记为ERROR，不再继续往下走了。 每次检测的逻辑都调用相同的函数，具体逻辑如下图所示： 2.2 Nova Super ConductorSuper Conductor在创建虚拟机的流程其实和之前差不多，选个合适的节点（调度），然后，写入虚拟机相关的记录，然后，投递消息到选定的Compute节点进行虚拟机的创建。 在Cell v2场景，虚拟机的创建记录已经需要写入的子cell中，因此，conductor需要做的事，包括一下几个步骤： 进行调度，选出host。 根据host，通过host_mappings找到对应的cell 在对应的cell db中创建虚拟机记录，并且记录instances_mappings信息 通过cell_mappings来查找对应的cell的mq，然后投递到对应的cell中的compute 完成这些操作时，需要牵扯到3个关键的数据结构，我们来简单的看一下： host_mappings：记录了host和cell的映射信息 instances_mappings：记录了虚拟机和cell的映射信息 cell_mappings：记录了cell和cell对应的mq的映射信息 与Cell v1不太相同，在目前的设计中，认为scheduler能看到的应该是底下能够提供资源的具体的所有的Resource Provider（对于计算资源来说，就是所有的计算节点），而不是整个cell，也就是说所有cell中的资源scheduler都可以看到，而子cell就负责创建就好了。因此，在super conductor中，需要做一些transfer的事情，这样也就不必在像cell v1那样，在子cell里还得搞个scheduler去做调度。 2.3 Nova-Scheduler刚才我们在conductor中，已经介绍了，在选择具体哪个节点来创建虚拟机时，调用了Scheduler的select_destination方法，在之前的版本的调度中，就是OpenStack最经典的Filter&amp;Weight的调度，已经有大量的资料介绍过具体的实现和用法。可以参考官方文档Filter Scheduler。 在Pike版本中，在调度这部分还是做了比较大的调度，主要就是2个相关变动。 通过Placement获取可用的备选资源，参考Placement Allocation Requests的实现。在Ocata版本时，Resource Providers - Scheduler Filters in DB这个BP就已经在调度前加了一步，获取备选节点。从BP的标题就可以看出，设计者想通过Placement服务提供的新的一套机制，来做过滤。原因是之前的调度需要在scheduler维护每一个compute节点的hoststate信息，然后调度的时候，再一个个去查，这太低效了，尤其是在计算节点数目比较多的时候。因此，增加了一个“预过滤”的流程，通过向Placement查询，Placement服务直接通过SQL去查一把，把满足条件（比如CPU充足、RAM充足等）先获取到。而原来获取备选节点的时候，只支持获取单一的Resource Provider，这个BP增强了获取备选资源的能力，用于后续支持更复杂的请求，比如共享资源、嵌套资源的Provider查询。后面，Placement还会陆续支持更多的请求，比如对一些非存量不可计数的资源的支持。这样留给后面Filter&amp;Weight的压力就小一些了，再往后，会不会完全取代Filter呢？我想，现有的各种过滤都可以通过Placement支持后，完全有可能的。 Scheduler通过Placement来claim资源。参考Scheduler claiming resources to the Placement API的实现。在最早的时候，claim资源是由compute来做的，现在相当于提前到scheduler去搞了。有什么好处呢？我们先看看原来的问题：调度时刻和真正的去compute节点去claim资源的时刻之间是由一段时间的，在资源不是那么充足的环境，就会造成在scheduler调度的时候，资源还没刷新，所以调度时候成功了，但是真正下来的时候，才发现compute实际已经没有资源了，然后又“跨越半个地球”去做重调度，无形地增加了系统的负载。而且增加了创建的时长（哦，哪怕创建失败呢？），你想想，用户创了那么久的虚拟机，最后你告诉我调度失败了，用户不太能忍。所以这个BP就把Claim资源放在调度处了，我上一个调度请求处理完，马上就告诉placement，这资源老子用了，其他人不要动了。OK，世界终于清净了，能拿到资源的拿到了，拿不到资源的马上也知道自己拿不到了，大大增强了调度的用户体验。 2.4 Placement恩，在调度的时候，已经介绍过这个服务了，在虚拟机创建的流程中，比较常用的接口就是获取备选资源和claim资源。Placement目标很宏伟，大致的作用就是：资源我来管，要资源问我要，用了资源告诉我。后面准备用一篇文章整体介绍一下Placement。（yep，这个Flag我立下了，会写的） 2.5 Nova-Compute好吧，到最后一个服务了，Compute。这个里面依旧还是做那么几件事，挂卷，挂网卡，调driver的接口启动一下虚拟机。至此，我们可爱的虚拟机就起来了。 3. 结语整体的看一下，其实在Pike版本，Nova还是有很多的变动。真的是一个版本过去了，创建虚拟机的流程已经面目全非了。 从P版本的虚拟机创建流程来看，主要的优化集中在基于Cell V2架构下的多cell支持、调度的优化、Quota的优化，而后续的发展，目前也是集中在Placement各种资源的支持以及在Cell v2场景的诸如基本流程、调度等的优化。","pubDate":"Wed, 27 Sep 2017 03:15:15 GMT","guid":"http://yikun.github.io/2017/09/27/OpenStack-Nova虚拟机创建流程解析/","category":"Nova,OpenStack"},{"title":"[译] Simpler Road to Cinder Active-Active","link":"http://yikun.github.io/2017/08/16/译-Simpler-Road-to-Cinder-Active-Active/","description":"译注：本篇文章为作者介绍Cinder AA方案的文章，作者是gorka，是实现cinder AA BP的core，文章介绍了这哥们实现AA时的记录，算是对方案的一种解释以及设计思路的总结，核心思想为以下几点： 每个volume node都增加一个cluster的配置项，作为集群，标记这个节点属于某个集群； 通过cluster@backend作为消息队列的topic，并且启动cluster@backend的服务； scheduler进行调度时，投递到某个合适的集群，集群中的某个后端进行消费； 消费时，将操作记录在worker中，用来标记这个资源由某个worker来操作，这样当发生异常时，可以确保仅有某个worker进行cleanup的操作。 原文链接：Simpler Road to Cinder Active-Active","pubDate":"Wed, 16 Aug 2017 12:50:32 GMT","guid":"http://yikun.github.io/2017/08/16/译-Simpler-Road-to-Cinder-Active-Active/","category":"OpenStack,Cinder"},{"title":"一次有关OpenStack请求的性能问题分析","link":"http://yikun.github.io/2016/07/22/一次有关OpenStack请求的性能问题分析/","description":"0. 背景介绍目前OpenStack对外提供的北向接口是以REST接口提供的，也就是说通过HTTP（HTTPS）接口进行请求，进行虚拟机或者卷等相关的操作。OpenStack提供I层基本的能力，比如创建、查询、删除虚拟机或者卷等操作，以OpenStack作为平台，对上提供用户接口，对下操作下层Driver完成对设备的操作，其大致的架构基本如下所示：","pubDate":"Fri, 22 Jul 2016 15:09:25 GMT","guid":"http://yikun.github.io/2016/07/22/一次有关OpenStack请求的性能问题分析/","category":"OpenStack"},{"title":"一致性哈希算法的理解与实践","link":"http://yikun.github.io/2016/06/09/一致性哈希算法的理解与实践/","description":"0. 概述在维基百科中，是这么定义的 一致哈希是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对 K/n个关键字重新映射，其中K是关键字的数量， n是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。 1. 引出 我们在上文中已经介绍了一致性Hash算法的基本优势，我们看到了该算法主要解决的问题是：当slot数发生变化时，能够尽量少的移动数据。那么，我们思考一下，普通的Hash算法是如何实现？又存在什么问题呢？那么我们引出一个问题： 假设有1000w个数据项，100个存储节点，请设计一种算法合理地将他们存储在这些节点上。 看一看普通Hash算法的原理：","pubDate":"Thu, 09 Jun 2016 02:43:54 GMT","guid":"http://yikun.github.io/2016/06/09/一致性哈希算法的理解与实践/","category":"系统"},{"title":"理解Python中的“with”","link":"http://yikun.github.io/2016/04/15/理解Python中的“with”/","description":"1. 缘起Python中，打开文件的操作是非常常见的，也是非常方便的，那么如何优雅的打开一个文件？大部分的同学会这样实现： 12with open( \"a.txt\" ) as f : # do something 大家都知道，这样写可以自动处理资源的释放、处理异常等，化简了我们打开文件的操作，那么，with到底做了什么呢？","pubDate":"Fri, 15 Apr 2016 15:44:15 GMT","guid":"http://yikun.github.io/2016/04/15/理解Python中的“with”/","category":"Python"},{"title":"存储数据包的一生","link":"http://yikun.github.io/2016/04/03/存储数据包的一生/","description":"最近认认真真学习了一个叫《Life of a Storage Packet》讲座，借助这个讲座将整个存储的过程理解了下，不放过任何一个有疑问的点。这篇文章算是对讲座的理解和自己收获的总结，同时也为那些对存储系统不够了解又想要了解的初学者，展现一个存储数据包的“生命”。这个演讲主要聚焦在“整体的存储”，强调存储系统中各个基本元素的关系，并且尽可能简单、清楚地用一种不同的方式可视化一些存储的概念。 先上一张大图，可以说这篇文章目的就是解释这个图：","pubDate":"Sun, 03 Apr 2016 14:04:23 GMT","guid":"http://yikun.github.io/2016/04/03/存储数据包的一生/","category":"系统"},{"title":"OpenStack源码分析-Cinder中的调度机制","link":"http://yikun.github.io/2016/03/05/OpenStack源码分析-Cinder中的调度机制/","description":"整理了一下目前cinder中支持的调度的Filter和Weigher：后面结合源码看下实现，留坑~","pubDate":"Fri, 04 Mar 2016 16:45:44 GMT","guid":"http://yikun.github.io/2016/03/05/OpenStack源码分析-Cinder中的调度机制/","category":"Cinder"},{"title":"OpenStack源码分析-Service启动流程","link":"http://yikun.github.io/2016/03/05/OpenStack源码分析-Service启动流程/","description":"","pubDate":"Fri, 04 Mar 2016 16:38:21 GMT","guid":"http://yikun.github.io/2016/03/05/OpenStack源码分析-Service启动流程/","category":"OpenStack,Cinder"},{"title":"OpenStack源码分析-挂载卷流程","link":"http://yikun.github.io/2016/03/05/OpenStack源码分析-挂载卷流程/","description":"1. 挂卷流程 当Nova volume-attach server volume执行后，主要经过以下几步：a. Nova Client解析指令，通过RESTFUL接口访问nova-api；b. Nova API解析响应请求获取虚拟机的基本信息，然后向cinder-api发出请求保留，并向nova-compute发送RPC异步调用请求卷挂载；c. Nova-compute向cinder-api初始化信息，并根据初始化连接调用Libvirt的接口完成挂卷流程；d. 进而调用cinder-volume获取连接，获取了连接后，通过RESTFUL请求cinder-api进行数据库更新操作。","pubDate":"Fri, 04 Mar 2016 16:32:58 GMT","guid":"http://yikun.github.io/2016/03/05/OpenStack源码分析-挂载卷流程/","category":"Cinder"},{"title":"优雅地调试OpenStack","link":"http://yikun.github.io/2016/02/23/优雅地调试OpenStack/","description":"恩，题目首先要起的高逼格一些。2333。 在前面学习代码的过程中，主要通过源码来学习，开始学起来确实有点费劲，因为欠缺对OpenStack的整体的意识，于是搭建OpenStack开发环境对OpenStack的运行环境和使用有了初步认知。也看到了启动OpenStack后的一些相关进程，那么这些进程是如何与源码对应起来的呢？如何去调试OpenStack呢？本篇文章就讲下我的探索。","pubDate":"Mon, 22 Feb 2016 16:00:52 GMT","guid":"http://yikun.github.io/2016/02/23/优雅地调试OpenStack/","category":"OpenStack"},{"title":"OpenStack源码分析-Cinder删除卷流程","link":"http://yikun.github.io/2016/02/21/OpenStack源码分析-Cinder删除卷流程/","description":"1. Cinder删除卷整体流程 删除卷流程比较简单，主要就是cinder-api解析Cilent的指令，并响应，发送RPC调用cinder-volume的delete操作，详细流程如下：a. Client发送删除指令，通过RESTful接口访问cinder-api；b. Cinder-api解析响应请求，通过RPC调用cinder-volume；c. Cinder-volume通过调用Driver的delete函数进行删除。 2. 源码详解 2.1 Cinder API(1) Cinder\\api\\v2\\volumes.pyVolumeController的delete函数响应请求，首先从API获取Volume对象信息，然后，调用API的delete对对象进行删除；(2) Cinder\\volume\\api.pyAPI.delete的对卷的状态进行检查，并更新状态为“deleting”，然后调用rpcapi的delete_volume函数 2.2 Cinder Volume(1) Cinder\\volume\\rpcapi.pyVolumeAPI函数投递一个远程消息，通过消息队列远程调用cinder volume的delete_volume函数。(2) Cinder\\volume\\manager最终通过VolumeManager调用dirver的delete_volume对卷进行删除。","pubDate":"Sun, 21 Feb 2016 11:56:59 GMT","guid":"http://yikun.github.io/2016/02/21/OpenStack源码分析-Cinder删除卷流程/","category":"OpenStack,Cinder"},{"title":"OpenStack源码分析-Cinder创建卷流程","link":"http://yikun.github.io/2016/02/14/OpenStack源码分析-Cinder创建卷流程/","description":"1. Cinder创卷整体流程 如整体架构图所示，创建卷涉及的答题步骤主要有以下几步：a. Client发送请求，通过RESTFUL接口访问cinder-api。b. Api解析响应请求，api解析由Client发送来的请求，并通过rpc进一步调用cinder-scheduler。c. Scheduler对资源进行调度，scheduler选择合适的节点进行。d. Volume调用Driver创建卷，volume通过指定Driver进行卷的创建。 2. 源码详解代码的整体流程如下所示： 从上图可以看出，整体处理流程包括三大部分，分别是API、Scheduler、Volume三部分。 2.1 Cinder API部分 (1) cinder\\api\\v2\\volumes.pyVolumeController. create函数对创建请求进行响应，首先函数对volume_type、metadata、snapshot等信息进行检查，然后调用Volume API的create进行创建。(2) cinder\\volume\\api.pyAPI.create函数对source_volume、volume_type等参数进行进一步检查，并调用cinder.volume.flows.api.get_flow来创建。(3) cinder\\volume\\flows\\api\\create_volume.pyget_flow函数检查Quata，最后创建EntryCreateTask及VolumeCastTask等任务，其中EntryCreateTask会将卷的创建过程写入数据库，此时卷的状态为”creating”。VolumeCastTask.excute函数会调用VoumeCastTask._cast_create_volumeVolumeCastTask._cast_create_volume函数，如果未传入host，则会经过调度进行创建卷，通过scheduler_rpcapi.create_volume创建卷；如果未传入host则直接交由Volume Manager去创建卷。 至此为止，Cinder API部分完成了自己的工作。 2.2 Cinder Scheduler (1) cinder\\scheduler\\rpcapi.py（此步还属于cinder-api）SchedulerAPI.create_volume函数会通过消息异步调用SchedulerManager.create_volume函数。(2) cinder\\scheduler\\manager.pySchedulerManager.create_volume函数，使用自己的flow来创建volume，其中还传入了Driver。(3) cinder\\scheduler\\flows\\create_volume.pyget_flow函数，创建ScheduleCreateVolumeTaskScheduleCreateVolumeTask.execute函数，会调用driver_api.schedule_create_volume(4) cinder\\scheduler\\filter_scheduler.pyFilterScheduler. schedule_create_volume函数，更新数据库，最后通过消息队列请求调用volume_rpcapi.create_volume。 2.3 Cinder Volume (1) /cinder/volume/rpcapi.py（此步还属于cinder-scheduler）VolumeAPI.create_volume会通过消息队列远程调用VolumeManager.create_volume(2) /cinder/volume/manager.pyVolumeManager函数也使用flow来创建volume，执行CreateVolumeFromSpecTask这个任务(3) /cinder/volume/flows/manager/create_volume.pyCreateVolumeFromSpecTask.excute，这个函数会根据创建的不同类别，去创建卷，例如调用create_raw_volume，最终会调用具体的driver进行卷的创建。在完成创卷后，CreateVolumeOnFinishTask这个任务，启动更新数据库，将卷更新为available状态。 我们可以看到在创建卷的过程中盘的状态会从“creating”状态变为“available”状态。","pubDate":"Sun, 14 Feb 2016 09:43:30 GMT","guid":"http://yikun.github.io/2016/02/14/OpenStack源码分析-Cinder创建卷流程/","category":"OpenStack,Cinder"},{"title":"搭建OpenStack开发环境","link":"http://yikun.github.io/2016/02/10/搭建OpenStack开发环境/","description":"前段时间主要了解了一些OpenStack相关的基础性东西，现在希望通过安装使用来增强一下对系统整体的认识，最近也读了一篇文章如何学习开源项目，基本和我的想法很类似，所以基本上也就是按照这个节奏来的。不说废话了，开始。","pubDate":"Tue, 09 Feb 2016 16:10:09 GMT","guid":"http://yikun.github.io/2016/02/10/搭建OpenStack开发环境/","category":"OpenStack"},{"title":"存储知识学习","link":"http://yikun.github.io/2016/02/03/存储知识学习/","description":"1. 磁盘基本知识磁盘大致由盘片、磁头、步进电机等几部分组成组成。盘面：硬盘一般含有一个或多个盘片，一个盘片包含两个盘面。磁道：每个盘面被划成多个狭窄的同心圆环，这样的圆环叫做磁道。扇区：每个磁道的每段圆弧叫做一个扇区，是读写的最小单位。柱面：所有盘面上的同一磁道，在竖直方向构成一个圆柱，称为柱面。 读写过程：硬盘读取数据时，磁头先移动到读取扇区所在磁道的上方，这个过程耗时叫做磁盘寻道时间，平均时间为10ms。之后，通过盘片的旋转，使得扇区转到磁头的下方，这个过程耗时叫做旋转延迟时间，对于7200转/min的硬盘转一周为60*1000/7200=8.33ms，平均旋转延迟为4.17ms（半圈）。 2. RAID基本知识RAID（Redundant Array of Independent Disks），即由独立的磁盘组成的具有冗余特性的阵列。其基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 0，条带化存储，容量增加，并行化，但无冗余，容易单点故障。 RAID 1，镜像存储，写入速率慢，读取速率快，有冗余备份，优点是高可靠、高可用，缺点是高花费。 RAID 2，RAID 0的改进版，使用汉明码进行检测和纠错，适用于连续IO、大块IO（如视频流）。 RAID 3，RAID 3和RAID 2的思路比较相似，使用奇偶校验进行错误检测和纠错，但校验盘单点故障。 RAID 4，RAID 4和RAID 3思路一样，只不过是使用BLOCK进行存储。 RAID 5，校验信息交叉的存储在所有数据盘上，高冗余，高数据传输率，实现复杂。 RAID 6，相比RAID5增加块内的校验，允许同时坏2块硬盘而不丢失数据。 RAID 01，先做条带（0），再做镜像（1）。读写速度快，数据保护能力强，空间利用率50%。RAID 10，先做镜像（1），再做条带（0）。 3. 存储方式根据网上的资料和理解，用Visio整理了一张图对比了下几种方式： DAS全称为Direct Attached Storage，即服务器直连存储。如图所示，文件系统直接通过RAID完成对硬件访问。优点是操作简便，经济，缺点是分散式存储，不可集中管理。NAS全称为Network Attached Storage，即网络存储服务。如图所示，文件系统通过网络暴露出来给应用服务。优点是结构简单。配置使用管理非常方便，可实现跨平台的数据共享。缺点是需要占用网络资源、应用局限性大。SAN全称为Storage Aera Network，即存储区域网络，如图所示，RAID接口通过网络暴露出来。优点是扩展性强，集中管理，缺点是成本较高，管理维护难度大。 4. IP SAN与FC SANFC SAN指基于光纤通道（Fiber Channel）的存储区域网，在FC SAN中存在两张网，一张面向应用的网（IP网），另一张中则是存储网（FC网）。而IP SAN的出现则是为了寻求一种新的方式，用与应用网相同的体系架构来构造存储网，使用通用的IP网络及设备。FC SAN性能好，价格高，但与主流的IP网络异构。适用于关键应用的几种存储、备份及容灾。IP SAN则由于以太网MTU（1518字节）的限制，性能稍差，但基于通用的IP协议。适用于异地间的数据交换、备份容灾，非关键应用的集中存储。 5. LVM基本知识LVM的全称是Logical Volume Manager，逻辑卷轴管理，主要解决的问题是，弹性调整文件系统的容量。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的存储，通过在磁盘分区和文件系统之间增加一个逻辑层，提供一个抽象的逻辑盘卷。 参考资料 《大话存储》 RAID技术介绍和总结http://blog.jobbole.com/83808/ 基于OpenStack的NAS服务https://www.ustack.com/blog/openstack-nas/","pubDate":"Wed, 03 Feb 2016 14:46:58 GMT","guid":"http://yikun.github.io/2016/02/03/存储知识学习/","category":"OpenStack,Cinder"},{"title":"[译]Internationalization","link":"http://yikun.github.io/2016/01/23/译-Internationalization/","description":"Nova uses the oslo.i18n library to support internationalization. The oslo.i18n library is built on top of gettext and provides functions that are used to enable user-facing strings such as log messages to appear in the appropriate language in different locales. Nova exposes the oslo.i18n library support via the nova/i18n.py integration module. This module provides the functions needed to wrap translatable strings. It provides the _() wrapper for general user-facing messages and specific wrappers for messages used only for logging. DEBUG level messages do not need translation but CRITICAL, ERROR, WARNING and INFO messages should be wrapped with _LC(), _LE(), _LW() or _LI() respectively. 理解：Nova是通过oslo.i18n来支持国际化的，oslo.i18n是基于getnext做的，这个库可以把面向用户的字符（比如日志）翻译成指定的语言。其中DEBUG信息不翻译，其他的信息会被翻译。 比如： 1234567# debug logLOG.debug(\"block_device_mapping %(mapping)s\", &#123;'mapping': block_device_mapping&#125;)# warn logLOG.warn(_LW('Unknown base file %(img)s'), &#123;'img': img&#125;)# not lograise nova.SomeException(_('Invalid service catalogue')) Do not use locals() for formatting messages because: 1. It is not as clear as using explicit dicts. 2. It could produce hidden errors during refactoring. 3. Changing the name of a variable causes a change in the message. 4. It creates a lot of otherwise unused variables. If you do not follow the project conventions, your code may cause hacking checks to fail. 另外，文中提到了不要使用locals()去格式化消息主要4点原因：1.不清楚是否有关键字. 2.重构时会有潜在的出错可能. 3.变量名变了消息就变. 4.创建很多无用的变量。 这些函数_(), _LC(), _LE(), _LW() and _LI()可以通过以下方法导入： 12345from nova.i18n import _from nova.i18n import _LCfrom nova.i18n import _LEfrom nova.i18n import _LWfrom nova.i18n import _LI","pubDate":"Fri, 22 Jan 2016 16:43:37 GMT","guid":"http://yikun.github.io/2016/01/23/译-Internationalization/","category":"Nova"},{"title":"[译]Virtual Machine States and Transitions","link":"http://yikun.github.io/2016/01/20/译-Virtual-Machine-States-and-Transitions/","description":"虚拟机的状态及其转移，主要讲了一些虚拟机的状态以及在创建虚拟机时的状态转移情况，目前理解不够深刻，需要在后面看代码时，进一步深入理解。","pubDate":"Wed, 20 Jan 2016 15:46:59 GMT","guid":"http://yikun.github.io/2016/01/20/译-Virtual-Machine-States-and-Transitions/","category":"Nova"},{"title":"2015，再见","link":"http://yikun.github.io/2016/01/02/2015，再见/","description":"2015年是很特殊的一年，是长达快20年的学生时代的终结。从年初最开始的时候，就对今年的会发生的事情做好了准备，也基本上按照自己的想法发生了。之前，也有一些计划，基本达到预期，现在想起这些目标都算是一些学生时代的梦想吧，这篇总结也大致的从这些方面展开。","pubDate":"Fri, 01 Jan 2016 16:05:42 GMT","guid":"http://yikun.github.io/2016/01/02/2015，再见/","category":"随笔"},{"title":"Python3源码学习-整型","link":"http://yikun.github.io/2015/12/21/Python3源码学习-整型/","description":"1. 引入我们先看看对整型变量i进行赋值，并对i进行显示的过程： 123&gt;&gt;&gt; i=1&gt;&gt;&gt; i1","pubDate":"Mon, 21 Dec 2015 13:48:29 GMT","guid":"http://yikun.github.io/2015/12/21/Python3源码学习-整型/","category":"Python"},{"title":"Python3源码学习-类型","link":"http://yikun.github.io/2015/12/20/Python3源码学习-类型/","description":"1. 类型我们在《Python3源码学习-对象》中提到了每个对象都含有一个type的属性，我们看看type是个什么东西，目光移到object.h： 1234567891011121314151617181920typedef struct _typeobject &#123; PyObject_VAR_HEAD const char *tp_name; /* For printing, in format \"&lt;module&gt;.&lt;name&gt;\" */ Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */ /* Methods to implement standard operations */ destructor tp_dealloc; //... ... /* More standard operations (here for binary compatibility) */ hashfunc tp_hash; ternaryfunc tp_call; reprfunc tp_str; getattrofunc tp_getattro; setattrofunc tp_setattro; //... ...&#125; PyTypeObject;","pubDate":"Sun, 20 Dec 2015 15:03:21 GMT","guid":"http://yikun.github.io/2015/12/20/Python3源码学习-类型/","category":"Python"},{"title":"Python3源码学习-编译Python源码","link":"http://yikun.github.io/2015/12/20/Python3源码学习-编译Python源码/","description":"在进行源码学习的时候，“实践出真知”。因此，在进行源码学习的过程中，我们首先需要对源码进行编译，然后，对我们感兴趣的点进行log，甚至debug。本篇文章记录了我在进行Python 3.5.0源码编译时的一些过程。","pubDate":"Sun, 20 Dec 2015 12:36:58 GMT","guid":"http://yikun.github.io/2015/12/20/Python3源码学习-编译Python源码/","category":"Python"},{"title":"Python3源码学习-对象","link":"http://yikun.github.io/2015/12/03/Python3源码学习-对象/","description":"最近开始看Python源码，大致看了看，发现Py2和Py3的部分实现差别挺大，《Python源码剖析》是根据Python 2写的。不过为了能激发主动性，便直接从Python 3（3.5.0）源码看起了，然后也会结合Python 2（2.7.10）的代码看看之前的实现，来对比学习~：） 1. 万物皆对象在Python中，万物皆对象，那么对象又是什么结构，如何组织，怎样实现的呢？","pubDate":"Thu, 03 Dec 2015 07:07:19 GMT","guid":"http://yikun.github.io/2015/12/03/Python3源码学习-对象/","category":"Python"}]}